{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "!pip install -q segmentation-models-pytorch\n",
    "!pip install -q torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "%pip install torchsummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface-hub==0.16.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out old / user-site installs that can shadow your env\n",
    "%pip uninstall -y timm segmentation-models-pytorch pretrained-backbones-unet\n",
    "\n",
    "# Fresh install known-good versions into THIS env (no cache / force)\n",
    "%pip install --no-cache-dir --force-reinstall \"timm>=0.9.8,<0.10\" \"segmentation-models-pytorch>=0.3.4\"\n",
    "\n",
    "# Verify\n",
    "import timm, timm.data.config as cfg\n",
    "print(\"timm version:\", timm.__version__)\n",
    "print(\"timm path:\", timm.__file__)\n",
    "print(\"has resolve_model_data_config:\", hasattr(cfg, \"resolve_model_data_config\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out old / user-site installs that can shadow your env\n",
    "%pip uninstall -y timm segmentation-models-pytorch pretrained-backbones-unet\n",
    "\n",
    "# Fresh install known-good versions into THIS env (no cache / force)\n",
    "%pip install --no-cache-dir --force-reinstall \"timm>=0.9.8,<0.10\" \"segmentation-models-pytorch>=0.3.4\"\n",
    "\n",
    "# Verify\n",
    "import timm, timm.data.config as cfg\n",
    "print(\"timm version:\", timm.__version__)\n",
    "print(\"timm path:\", timm.__file__)\n",
    "print(\"has resolve_model_data_config:\", hasattr(cfg, \"resolve_model_data_config\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48rFWt8_m5_a",
    "outputId": "08e112e4-7040-431b-eafa-a6149257af37"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing cv2: The operating system cannot run %1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mA\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing cv2: The operating system cannot run %1."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#!pip install -q segmentation-models-pytorch\n",
    "#!pip install -q torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "#print(os.getenv('PATH'))\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing cv2: The operating system cannot run %1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m; \u001b[38;5;28mprint\u001b[39m(cv2\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing cv2: The operating system cannot run %1."
     ]
    }
   ],
   "source": [
    "import cv2; print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.cuda.current_device())  # Should return the current GPU device\n",
    "print(torch.cuda.get_device_name(0))  # Should return the name of the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ✨ Training Logger + Best-Model + Extra Metrics (Paste as one cell) ===\n",
    "# Usage:\n",
    "#   1) Run this cell once near the top (after imports).\n",
    "#   2) Wrap your training loop with the helper below:\n",
    "#        logger = TrainingLogger(run_dir=\"runs/unet_alaska\")\n",
    "#        for ep in range(epoch):\n",
    "#            # ... your train epoch ...\n",
    "#            logger.log_epoch(ep, \"train\", train_loss, train_mIoU, train_acc, optimizer)\n",
    "#            # ... your val epoch ...\n",
    "#            logger.log_epoch(ep, \"val\",   val_loss,   val_mIoU,   val_acc,   optimizer)\n",
    "#            # Save-best:\n",
    "#            if val_mIoU > logger.best_miou:\n",
    "#                logger.best_miou = val_mIoU\n",
    "#                logger.save_best(model, f\"Unet-mobilenet2-best_mIoU-{val_mIoU:.3f}.pt\")\n",
    "#        logger.close()\n",
    "#\n",
    "#   3) Ensure OneCycleLR is configured with steps_per_epoch=len(train_loader).\n",
    "#\n",
    "# Optional:\n",
    "#   - Use dice_coefficient_binary(logits, mask) to track Dice for foreground.\n",
    "#   - Use count_params(model) for parameter count.\n",
    "#\n",
    "import os, time, numpy as np, torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True); return path\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "@torch.no_grad()\n",
    "def dice_coefficient_binary(logits, target, eps: float = 1e-7):\n",
    "    \\\"\\\"\\\"Dice on foreground for a 2-class CE setup.\n",
    "    logits: (B,2,H,W), target: (B,H,W) with {0,1}\n",
    "    \\\"\\\"\\\"\n",
    "    pred = logits.argmax(1)\n",
    "    inter = ((pred == 1) & (target == 1)).sum().item()\n",
    "    denom = (pred == 1).sum().item() + (target == 1).sum().item()\n",
    "    return (2 * inter + eps) / (denom + eps)\n",
    "\n",
    "class TrainingLogger:\n",
    "    def __init__(self, run_dir=\\\"runs/unet_run\\\", flush_secs=10):\n",
    "        ensure_dir(run_dir)\n",
    "        self.writer = SummaryWriter(run_dir, flush_secs=flush_secs)\n",
    "        self.best_miou = -1.0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        if value is not None and np.isfinite(value):\n",
    "            self.writer.add_scalar(tag, float(value), step)\n",
    "\n",
    "    def log_epoch(self, epoch, phase, loss, miou, acc, optimizer=None):\n",
    "        assert phase in {\\\"train\\\", \\\"val\\\"}\n",
    "        self.log_scalar(f\\\"Loss/{phase}\\\", loss, epoch)\n",
    "        self.log_scalar(f\\\"mIoU/{phase}\\\", miou, epoch)\n",
    "        self.log_scalar(f\\\"Acc/{phase}\\\",  acc,  epoch)\n",
    "        if optimizer is not None and optimizer.param_groups:\n",
    "            lr = optimizer.param_groups[0].get(\\\"lr\\\", None)\n",
    "            self.log_scalar(\\\"LR\\\", lr, epoch)\n",
    "\n",
    "    def save_best(self, model, path=\\\"best_model.pt\\\"):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "print(\\\"✅ Training logger utilities loaded. See the usage block at the top of this cell.\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths to the train, valid, and test image and mask folders\n",
    "# Example usage\n",
    "main_folder = r'D:\\ML_Seagrass\\SourceData\\Alaska\\Alaska'\n",
    "\n",
    "train_image_folder = os.path.join(main_folder, 'train', 'image')\n",
    "train_mask_folder = os.path.join(main_folder, 'train', 'index')\n",
    "val_image_folder = os.path.join(main_folder, 'valid', 'image')\n",
    "val_mask_folder = os.path.join(main_folder, 'valid', 'index')\n",
    "test_image_folder = os.path.join(main_folder, 'test', 'image')\n",
    "test_mask_folder = os.path.join(main_folder, 'test', 'index')\n",
    "\n",
    "train_glcm_folder = os.path.join(main_folder, 'train', 'glcm')\n",
    "valid_glcm_folder = os.path.join(main_folder, 'valid', 'glcm')\n",
    "    \n",
    "# Function to create dataframes from the given folder structure\n",
    "def create_dataframes_from_main_folder(main_folder):\n",
    "    \n",
    "\n",
    "    # Helper function to create a dataframe from image and mask folders\n",
    "    def create_dataframe_from_folders(image_folder, mask_folder):\n",
    "        image_names = [img.split('.')[0] for img in os.listdir(image_folder) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        mask_names = [mask.split('.')[0] for mask in os.listdir(mask_folder) if mask.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        # Ensure that the image and mask file names match\n",
    "        assert set(image_names) == set(mask_names), \"Image and mask file names do not match!\"\n",
    "\n",
    "        return pd.DataFrame({'id': image_names})\n",
    "\n",
    "    # Create dataframes for train, valid, and test sets\n",
    "    df_train = create_dataframe_from_folders(train_image_folder, train_mask_folder)\n",
    "    df_val = create_dataframe_from_folders(val_image_folder, val_mask_folder)\n",
    "    df_test = create_dataframe_from_folders(test_image_folder, test_mask_folder)\n",
    "\n",
    "    # Return dataframes\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "  # Replace with the path to your main folder\n",
    "df_train, df_val, df_test = create_dataframes_from_main_folder(main_folder)\n",
    "\n",
    "# Assign X_train, X_val, and X_test\n",
    "X_train = df_train['id'].values\n",
    "X_val = df_val['id'].values\n",
    "X_test = df_test['id'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "id": "wf0e_7LGpggP",
    "outputId": "8856ad38-5d4e-4b10-a11c-8494b69b46df"
   },
   "outputs": [],
   "source": [
    "for idx in X_val:\n",
    "    img_path = os.path.join(val_image_folder, idx + '.png')\n",
    "    mask_path = os.path.join(val_mask_folder, idx + '.png')\n",
    "\n",
    "    \n",
    "    # Print out the file paths for debugging\n",
    "    print(\"Image Path:\", img_path)\n",
    "    print(\"Mask Path:\", mask_path)\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.imshow(mask)\n",
    "    plt.show()\n",
    "    print(np.asarray(img).shape)\n",
    "    print(np.asarray(mask).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "class Drone_data(Dataset):\n",
    "    def __init__(self, img_path, mask_path, grayscale_path, X, mean, std, transform=None, patch=False):\n",
    "        self.img_path = Path(img_path)\n",
    "        self.mask_path = Path(mask_path)\n",
    "        self.grayscale_path = Path(grayscale_path)\n",
    "        self.X = X\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "        self.patch = patch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create image and mask paths using pathlib\n",
    "        img_path = self.img_path / f'{self.X[idx]}.png'\n",
    "        mask_path = self.mask_path / f'{self.X[idx]}.png'\n",
    "        grayscale_path = self.grayscale_path / f'{self.X[idx]}.png'\n",
    "        \n",
    "        # Read and convert the image to RGB\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load the additional grayscale image\n",
    "        grayscale_img = cv2.imread(str(grayscale_path), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Replace the red channel with the grayscale image\n",
    "        img[:, :, 0] = grayscale_img  # Replace the red channel (channel 0)\n",
    "        \n",
    "        # Open and convert the mask to grayscale\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = mask.convert(\"L\")\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # Map mask values: 255 -> 1\n",
    "        mask = np.where(mask == 255, 1, 0).astype(np.uint8)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = aug['image']\n",
    "            mask = aug['mask']\n",
    "\n",
    "        # Convert image to tensor and normalize\n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        img = t(img)\n",
    "        \n",
    "        # Convert mask to a tensor\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        if self.patch:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def tiles(self, img, mask):\n",
    "        # Implementation of tile extraction if needed\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRd3W4Ibpqv5"
   },
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "t_train = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(),\n",
    "                     A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n",
    "                     A.GaussNoise()])\n",
    "\n",
    "t_val = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(),\n",
    "                   A.GridDistortion(p=0.2)])\n",
    "\n",
    "#datasets\n",
    "train_set = Drone_data(\n",
    "    img_path=train_image_folder,\n",
    "    mask_path=train_mask_folder,\n",
    "    grayscale_path=train_glcm_folder,\n",
    "    X=X_train,\n",
    "    mean=[0.485, 0.456, 0.406],  # Example mean values\n",
    "    std=[0.229, 0.224, 0.225],   # Example std values\n",
    "    transform=t_train,    # Optional Albumentations transform\n",
    "    patch=False                  # Set to True if you need patch-based data\n",
    ")\n",
    "\n",
    "val_set = Drone_data(val_image_folder, val_mask_folder, valid_glcm_folder, X_val, mean, std, t_val, patch=False)\n",
    "\n",
    "#dataloader\n",
    "batch_size= 4\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sbi4cet3pr4s"
   },
   "outputs": [],
   "source": [
    "model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=2, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J04RHDRDpug2"
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=2):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xV71lNyp0MB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []; val_acc = []\n",
    "    train_iou = []; train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1 ; not_improve=0\n",
    "\n",
    "    logger = TrainingLogger(run_dir=\"runs/unet_alaska\")\n",
    "    best_mIoU = -1.0\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        #training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            #training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "\n",
    "                image_tiles = image_tiles.view(-1,c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "            image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "            #forward\n",
    "            output = model(image)\n",
    "            #output = torch.argmax(output, dim=1)\n",
    "            loss = criterion(output, mask)\n",
    "            #evaluation metrics\n",
    "            iou_score += mIoU(output, mask)\n",
    "            accuracy += pixel_accuracy(output, mask)\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step() #update weight\n",
    "            optimizer.zero_grad() #reset gradient\n",
    "\n",
    "            #step the learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            logger.log_epoch(e, \"train\", train_loss, train_mIoU, train_acc, optimizer)\n",
    "\n",
    "\n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            #validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    #reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "\n",
    "                    if patch:\n",
    "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
    "\n",
    "                        image_tiles = image_tiles.view(-1,c, h, w)\n",
    "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "                    image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "                    output = model(image)\n",
    "                    #evaluation metrics\n",
    "                    val_iou_score +=  mIoU(output, mask)\n",
    "                    test_accuracy += pixel_accuracy(output, mask)\n",
    "                    #loss\n",
    "                    loss = criterion(output, mask)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "            #calculatio mean for each batch\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(val_loader))\n",
    "\n",
    "\n",
    "            if min_loss > (test_loss/len(val_loader)):\n",
    "                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                decrease += 1\n",
    "                if decrease % 5 == 0:\n",
    "                    print('saving model...')\n",
    "                    torch.save(model, 'Unet-Mobilenet_v2_mIoU-{:.3f}.pt'.format(val_iou_score/len(val_loader)))\n",
    "\n",
    "\n",
    "            if (test_loss/len(val_loader)) > min_loss:\n",
    "                not_improve += 1\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                print(f'Loss Not Decrease for {not_improve} time')\n",
    "                if not_improve == 7:\n",
    "                    print('Loss not decrease for 7 times, Stop Training')\n",
    "                    break\n",
    "\n",
    "            #iou\n",
    "            val_iou.append(val_iou_score/len(val_loader))\n",
    "            train_iou.append(iou_score/len(train_loader))\n",
    "            train_acc.append(accuracy/len(train_loader))\n",
    "            val_acc.append(test_accuracy/ len(val_loader))\n",
    "            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n",
    "                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
    "                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n",
    "                  \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n",
    "                  \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n",
    "                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n",
    "                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n",
    "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "\n",
    "    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n",
    "               'train_miou' :train_iou, 'val_miou':val_iou,\n",
    "               'train_acc' :train_acc, 'val_acc':val_acc,\n",
    "               'lrs': lrs}\n",
    "    \n",
    "    total_time_minutes = (time.time() - fit_time) / 60\n",
    "\n",
    "    # Print the total time\n",
    "    print('Total time: {:.2f} m'.format(total_time_minutes))\n",
    "\n",
    "    #print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "    return history\n",
    "'''\n",
    "\n",
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False, n_classes=2, region=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses, test_losses = [], []\n",
    "    val_iou, val_acc = [], []\n",
    "    train_iou, train_acc = [], []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1\n",
    "    not_improve = 0\n",
    "\n",
    "    logger = TrainingLogger(run_dir=\"runs/unet_{region}}\")\n",
    "    logger.best_miou = -1.0  # track best val mIoU\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0.0\n",
    "        iou_score_sum = 0.0\n",
    "        acc_sum = 0.0\n",
    "\n",
    "        # ===== TRAIN =====\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            image_tiles, mask_tiles = data\n",
    "\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                mask_tiles  = mask_tiles.view(-1, h, w)\n",
    "\n",
    "            image = image_tiles.to(device)\n",
    "            mask  = mask_tiles.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # reset grad BEFORE forward\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, mask)\n",
    "\n",
    "            # metrics (per-batch)\n",
    "            iou_score_sum += mIoU(output, mask, n_classes=n_classes)\n",
    "            acc_sum       += pixel_accuracy(output, mask)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # step the learning rate\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            # track lr and loss\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # ===== after train epoch: compute averages for logging =====\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_mIoU = iou_score_sum / len(train_loader)\n",
    "        train_accuracy = acc_sum / len(train_loader)\n",
    "\n",
    "        # ===== VAL =====\n",
    "        model.eval()\n",
    "        test_loss_sum = 0.0\n",
    "        val_iou_sum = 0.0\n",
    "        val_acc_sum = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(val_loader)):\n",
    "                image_tiles, mask_tiles = data\n",
    "\n",
    "                if patch:\n",
    "                    bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                    image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                    mask_tiles  = mask_tiles.view(-1, h, w)\n",
    "\n",
    "                image = image_tiles.to(device)\n",
    "                mask  = mask_tiles.to(device)\n",
    "\n",
    "                output = model(image)\n",
    "                loss = criterion(output, mask)\n",
    "\n",
    "                test_loss_sum += loss.item()\n",
    "                val_iou_sum   += mIoU(output, mask, n_classes=n_classes)\n",
    "                val_acc_sum   += pixel_accuracy(output, mask)\n",
    "\n",
    "        val_loss = test_loss_sum / len(val_loader)\n",
    "        val_mIoU_epoch = val_iou_sum / len(val_loader)\n",
    "        val_accuracy   = val_acc_sum / len(val_loader)\n",
    "\n",
    "        # ===== history arrays =====\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(val_loss)\n",
    "        train_iou.append(train_mIoU)\n",
    "        val_iou.append(val_mIoU_epoch)\n",
    "        train_acc.append(train_accuracy)\n",
    "        val_acc.append(val_accuracy)\n",
    "\n",
    "        # ===== logging (now variables are defined) =====\n",
    "        logger.log_epoch(e, \"train\", train_loss, train_mIoU, train_accuracy, optimizer)\n",
    "        logger.log_epoch(e, \"val\",   val_loss,  val_mIoU_epoch, val_accuracy, optimizer)\n",
    "\n",
    "        # ===== best-by-IoU saver =====\n",
    "        if val_mIoU_epoch > logger.best_miou:\n",
    "            logger.best_miou = val_mIoU_epoch\n",
    "            logger.save_best(model, f\"Unet-{region}-best_mIoU-{val_mIoU_epoch:.3f}.pt\")\n",
    "\n",
    "        # ===== early stopping & occasional save-by-loss (your original behavior) =====\n",
    "        if val_loss < min_loss:\n",
    "            print('Loss Decreasing.. {:.3f} >> {:.3f}'.format(min_loss, val_loss))\n",
    "            min_loss = val_loss\n",
    "            decrease += 1\n",
    "            if decrease % 5 == 0:\n",
    "                print('saving model (periodic by loss)...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_gpu_memory():\n",
    "    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "    print(result.stdout.decode('utf-8'))\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7c1f12cb1ab847249bb4280c54b564cb",
      "19d6d94b89704a269debe53bb709576d",
      "d4f923059c214d2a8aa3fd74ef0b0b38",
      "be620ffd47314ef3b9e071b5f1eca94c",
      "163f38a04bda46ee92bb147e7d5faa80",
      "1a19a3eae622461585e820b2fb93316c",
      "66a3ea8eb4f24f50b8e82fb9f25e3bcd",
      "0d48ded90abf4b10b0ce4a023cedc338",
      "4907a67a924f4daea078544b5a929188",
      "110ee953de00408389437603ab9cbd15",
      "4fc823b122304890af9cb927f3f8afef"
     ]
    },
    "id": "58OHqNi0p2iv",
    "outputId": "21a4592b-8670-4713-e9b2-cd77b9969706"
   },
   "outputs": [],
   "source": [
    "region = 'Alaska'\n",
    "\n",
    "max_lr = 1e-4\n",
    "epoch = 20\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
    "#                                             steps_per_epoch=len(train_loader))\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=max_lr, epochs=epoch, steps_per_epoch=len(train_loader), epochs=epoch, pct_start=0.1\n",
    ")\n",
    "\n",
    "\n",
    "history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched, region)\n",
    "\n",
    "torch.save(model, 'Unet-mobilenet2-batchsize4-Alaska_lr-4-glcm-rep-red.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'C:\\Users\\Yang_PC\\Documents\\Github\\ArcgisPro-Customized-Toolbox\\Notebook\\Unet-mobilenet2-batchsize4-Alaska_lr-4-glcm-rep-red.pt'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(model_path)\n",
    "model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0tS5lEOFR-o"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history['val_loss'], label='val', marker='o')\n",
    "    plt.plot( history['train_loss'], label='train', marker='o')\n",
    "    plt.title('Loss per epoch'); plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_score(history):\n",
    "    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n",
    "    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n",
    "    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_acc(history):\n",
    "    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n",
    "    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n",
    "    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_trDOO9hFV8p",
    "outputId": "f8342968-3714-48f5-e510-976db45d5ee3"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_score(history)\n",
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7vAnGWxIcvF"
   },
   "outputs": [],
   "source": [
    "class DroneTestDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, X, transform=None):\n",
    "        self.img_path = Path(img_path)\n",
    "        self.mask_path = Path(mask_path)\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create image and mask paths using pathlib\n",
    "        img_file_name = f'{self.X[idx]}.png'\n",
    "        img_path = self.img_path / img_file_name\n",
    "        mask_path = self.mask_path / img_file_name\n",
    "        \n",
    "        # Read and convert the image to RGB using OpenCV\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Read the mask in grayscale directly using OpenCV\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # Convert image and mask to NumPy arrays for transformation\n",
    "            transformed = self.transform(image=img, mask=mask)\n",
    "            img = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        # Convert the image to PIL Image (consistent with DroneTestDataset1)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        # Convert mask to a PyTorch tensor\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        # Return the image, mask, and file name\n",
    "        return img, mask, img_file_name\n",
    "\n",
    "t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
    "test_set = DroneTestDataset(test_image_folder, test_mask_folder, X_test, transform=t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Updated function to perform inference and save the predicted mask with resizing\n",
    "def predict_image_mask_miou(model, image, mask, save_file, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Store original image dimensions before transforming\n",
    "    original_height, original_width = 512, 512  # PIL Image size: (width, height)\n",
    "    \n",
    "    # Apply transformation to the image (e.g., normalization)\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image_tensor = t(image)  # Keep 'image' unchanged for later use\n",
    "    \n",
    "    # Move image and mask to the appropriate device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension for the model\n",
    "        mask = mask.unsqueeze(0)    # Add batch dimension\n",
    "\n",
    "        output = model(image_tensor)  # Run model inference\n",
    "        score = mIoU(output, mask)  # Compute mIoU score\n",
    "        \n",
    "        # Get the predicted mask by taking the argmax of the model's output\n",
    "        pred_mask = torch.argmax(output, dim=1)\n",
    "        pred_mask = pred_mask.cpu().squeeze(0)  # Move to CPU and remove batch dimension\n",
    "\n",
    "    # Resize the predicted mask to match the original image size\n",
    "    pred_mask_resized = cv2.resize(pred_mask.numpy(), (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Save the resized predicted mask as a grayscale image\n",
    "    plt.imsave(save_file, pred_mask_resized, cmap='gray')  # Save mask as a grayscale image\n",
    "    \n",
    "    return pred_mask_resized, score\n",
    "\n",
    "\n",
    "\n",
    "def save_predicted_mask_overlay(pred_mask, img_file_name, image_folder, save_file, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlays the predicted mask on the original image read from a specified folder, and saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred_mask: The predicted mask (PyTorch tensor or NumPy array with values 0 and 1).\n",
    "    - img_file_name: The name of the image file (e.g., 'image_001.png').\n",
    "    - image_folder: The path to the folder where the original image is located.\n",
    "    - save_file: The path to save the overlaid image.\n",
    "    - alpha: The transparency factor for the overlay (default is 0.3 for higher transparency).\n",
    "    \"\"\"\n",
    "    # Build the full image path\n",
    "    img_path = os.path.join(image_folder, img_file_name)\n",
    "\n",
    "    # Read the original image from the specified folder\n",
    "    original_image = Image.open(img_path).convert('RGB')  # Use PIL to read the image\n",
    "    original_image = np.array(original_image)  # Convert to NumPy array\n",
    "\n",
    "    # Convert the predicted mask to a NumPy array if it's a PyTorch tensor\n",
    "    if isinstance(pred_mask, torch.Tensor):\n",
    "        pred_mask = pred_mask.cpu().numpy()\n",
    "\n",
    "    # Ensure the predicted mask has 2D shape (height, width)\n",
    "    if pred_mask.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D mask, but got shape {pred_mask.shape}\")\n",
    "\n",
    "    # Resize the predicted mask to match the original image size (assuming original image size is 512x512)\n",
    "    original_height, original_width = original_image.shape[:2]\n",
    "    pred_mask_resized = cv2.resize(pred_mask, (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Ensure the image is in the correct format (height, width, 3)\n",
    "    if original_image.ndim != 3 or original_image.shape[2] != 3:\n",
    "        raise ValueError(f\"Expected 3D RGB image, but got shape {original_image.shape}\")\n",
    "\n",
    "    # Create a copy of the original image to modify\n",
    "    overlay = original_image.copy()\n",
    "\n",
    "    # Broadcast the mask across the 3 RGB channels (set the mask area to light blue)\n",
    "    light_blue_color = [78,156,217]  # Light blue RGB\n",
    "    light_blue_mask = np.zeros_like(overlay)\n",
    "    light_blue_mask[pred_mask_resized == 1] = light_blue_color  # Light blue for class 1 (object)\n",
    "\n",
    "    # Blend the original image and the light blue mask, keeping the original image intensity intact\n",
    "    blended_image = cv2.addWeighted(light_blue_mask, alpha, original_image, 1, 0)\n",
    "\n",
    "    # Save the blended image\n",
    "    plt.imsave(save_file, blended_image)\n",
    "    print(f\"Saved overlay image: {save_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the save path for the predicted masks and the folder for original images\n",
    "original_image_folder = r'D:\\ML_Seagrass\\SourceData\\Alaska\\Alaska\\test\\image'  # Folder with original images\n",
    "save_path = r'D:\\ML_Seagrass\\SourceData\\Alaska\\Alaska\\test\\overlay'\n",
    "Pred_mask_path = r'D:\\ML_Seagrass\\SourceData\\Alaska\\Alaska\\test\\predicted'\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(Pred_mask_path, exist_ok=True)\n",
    "\n",
    "# Loop through all images in the test set\n",
    "for idx in tqdm(range(len(test_set))):\n",
    "    image, mask, img_file_name = test_set[idx]  # Get mask and filename from test_set\n",
    "    \n",
    "    # Define the save path for the current predicted mask\n",
    "    save_overlay_file = os.path.join(save_path, f'{img_file_name}')  # Save with filename\n",
    "    save_predicted_file = os.path.join(Pred_mask_path, f'{img_file_name}')\n",
    "     \n",
    "    # Run inference to get the predicted mask\n",
    "    pred_mask, score = predict_image_mask_miou(model, image, mask, save_predicted_file)\n",
    "    \n",
    "    # Save the overlay of the predicted mask on the original image read from `original_image_folder`\n",
    "    save_predicted_mask_overlay(pred_mask, img_file_name, original_image_folder, save_overlay_file)\n",
    "    \n",
    "    # Optionally, log the progress or save additional data\n",
    "    print(f\"Saved predicted mask for {img_file_name} with mIoU score: {score:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03a2141fad814fa39632b87e7ef8a134": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c6a036444b043ef9df7cc7e854a2a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d48ded90abf4b10b0ce4a023cedc338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0da6636154224e239e0acf128251552d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfea514454064155841bb153d837b5b3",
      "placeholder": "​",
      "style": "IPY_MODEL_7b58b04187ad4246919324f5fa3d9a44",
      "value": "100%"
     }
    },
    "110ee953de00408389437603ab9cbd15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "163f38a04bda46ee92bb147e7d5faa80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19d6d94b89704a269debe53bb709576d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a19a3eae622461585e820b2fb93316c",
      "placeholder": "​",
      "style": "IPY_MODEL_66a3ea8eb4f24f50b8e82fb9f25e3bcd",
      "value": "  0%"
     }
    },
    "1a19a3eae622461585e820b2fb93316c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c7b25ad1e4b43c79785da22f7abb982": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4907a67a924f4daea078544b5a929188": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a1b1fc61367429491b12299dc5d1ec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a3d1a2536ca421ba23300b348c7372d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0da6636154224e239e0acf128251552d",
       "IPY_MODEL_f2885ad7a0d946168987cd0860ce2a13",
       "IPY_MODEL_edf110fd6b7f48caa90967a684a7d784"
      ],
      "layout": "IPY_MODEL_1c7b25ad1e4b43c79785da22f7abb982"
     }
    },
    "4c552b1eb28649ec8c49645da3c72bc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e5b4dbac61640bc9dc8286cfee93f13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03a2141fad814fa39632b87e7ef8a134",
      "placeholder": "​",
      "style": "IPY_MODEL_c99b9ceff61144ca937598af9d150c8b",
      "value": "100%"
     }
    },
    "4fc823b122304890af9cb927f3f8afef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66a3ea8eb4f24f50b8e82fb9f25e3bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68dcb9ff28f54dd387d3e816473cd13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b58b04187ad4246919324f5fa3d9a44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c1f12cb1ab847249bb4280c54b564cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_19d6d94b89704a269debe53bb709576d",
       "IPY_MODEL_d4f923059c214d2a8aa3fd74ef0b0b38",
       "IPY_MODEL_be620ffd47314ef3b9e071b5f1eca94c"
      ],
      "layout": "IPY_MODEL_163f38a04bda46ee92bb147e7d5faa80"
     }
    },
    "84657db6d3dc46d5bd0fa25898ee14f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ee2363c68e54d908f2dfb0b698549d3",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f05c6fe1e5c04ebdae94c909041712ac",
      "value": 9
     }
    },
    "8834a7b9229842e78082e64a5157a0ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ee2363c68e54d908f2dfb0b698549d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9075c9929447407ea9e9c5288583dadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92b88d70c83248279bf530ea2041bfe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8834a7b9229842e78082e64a5157a0ce",
      "placeholder": "​",
      "style": "IPY_MODEL_0c6a036444b043ef9df7cc7e854a2a17",
      "value": " 9/9 [00:23&lt;00:00,  2.84s/it]"
     }
    },
    "a24b9d4c1e524a1c9c105e6e5740bde1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e5b4dbac61640bc9dc8286cfee93f13",
       "IPY_MODEL_84657db6d3dc46d5bd0fa25898ee14f2",
       "IPY_MODEL_92b88d70c83248279bf530ea2041bfe8"
      ],
      "layout": "IPY_MODEL_f7a8233fab1e4dff8bb2403538b49da1"
     }
    },
    "be620ffd47314ef3b9e071b5f1eca94c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_110ee953de00408389437603ab9cbd15",
      "placeholder": "​",
      "style": "IPY_MODEL_4fc823b122304890af9cb927f3f8afef",
      "value": " 0/11 [00:00&lt;?, ?it/s]"
     }
    },
    "bfea514454064155841bb153d837b5b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c99b9ceff61144ca937598af9d150c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4f923059c214d2a8aa3fd74ef0b0b38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d48ded90abf4b10b0ce4a023cedc338",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4907a67a924f4daea078544b5a929188",
      "value": 0
     }
    },
    "edf110fd6b7f48caa90967a684a7d784": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c552b1eb28649ec8c49645da3c72bc9",
      "placeholder": "​",
      "style": "IPY_MODEL_4a1b1fc61367429491b12299dc5d1ec9",
      "value": " 9/9 [00:01&lt;00:00,  6.16it/s]"
     }
    },
    "f05c6fe1e5c04ebdae94c909041712ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2885ad7a0d946168987cd0860ce2a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68dcb9ff28f54dd387d3e816473cd13f",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9075c9929447407ea9e9c5288583dadb",
      "value": 9
     }
    },
    "f7a8233fab1e4dff8bb2403538b49da1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
